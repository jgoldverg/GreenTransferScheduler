{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:25:22.685714Z",
     "start_time": "2025-05-11T15:25:21.604767Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /opt/homebrew/lib/python3.13/site-packages (4.13.2)\r\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.13/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: torch-geometric in /opt/homebrew/lib/python3.13/site-packages (2.6.1)\r\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.13/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.13/site-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/lib/python3.13/site-packages (from torch) (4.13.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.13/site-packages (from torch) (80.4.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.13/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.13/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.13/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.13/site-packages (from torch) (2025.3.2)\r\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.13/site-packages (from torch-geometric) (3.11.18)\r\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.13/site-packages (from torch-geometric) (2.2.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/homebrew/lib/python3.13/site-packages (from torch-geometric) (6.1.1)\r\n",
      "Requirement already satisfied: pyparsing in /opt/homebrew/lib/python3.13/site-packages (from torch-geometric) (3.2.3)\r\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.13/site-packages (from torch-geometric) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.13/site-packages (from torch-geometric) (4.67.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp->torch-geometric) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp->torch-geometric) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp->torch-geometric) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp->torch-geometric) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp->torch-geometric) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp->torch-geometric) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp->torch-geometric) (1.20.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests->torch-geometric) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.13/site-packages (from requests->torch-geometric) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests->torch-geometric) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.13/site-packages (from requests->torch-geometric) (2024.12.14)\r\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "!pip3 install --upgrade typing_extensions --break-system-packages\n",
    "!pip3 install torch torch-geometric pandas --break-system-packages"
   ],
   "id": "initial_id",
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:04:45.428462Z",
     "start_time": "2025-05-11T18:04:45.423904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ],
   "id": "da2223975175389c",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:04:45.815665Z",
     "start_time": "2025-05-11T18:04:45.804362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_graph(associations_df):\n",
    "    # Unique nodes\n",
    "    job_ids = sorted(associations_df['job_id'].unique())\n",
    "    route_keys = sorted(associations_df['route_key'].unique())\n",
    "\n",
    "    # Create mappings\n",
    "    job_id_map = {job_id: idx for idx, job_id in enumerate(job_ids)}\n",
    "    route_map = {route: idx + len(job_ids) for idx, route in enumerate(route_keys)}\n",
    "\n",
    "    # Node features\n",
    "    job_features = []\n",
    "    for job_id in job_ids:\n",
    "        job_data = associations_df[associations_df['job_id'] == job_id].iloc[0]\n",
    "        job_features.append([\n",
    "            float(job_data['source_cpu']),\n",
    "            float(job_data['source_ram']),\n",
    "            float(job_data['job_deadline']),\n",
    "            float(job_data['transfer_time_hours'])\n",
    "        ])\n",
    "\n",
    "    route_features = []\n",
    "    for route in route_keys:\n",
    "        route_data = associations_df[associations_df['route_key'] == route].iloc[0]\n",
    "        route_features.append([\n",
    "            float(route_data['transfer_time']),\n",
    "            float(route_data['throughput_gbps']),\n",
    "            float(route_data['carbon_emissions']),\n",
    "            float(route_data['source_nic_speed'] == '10Gbps')\n",
    "        ])\n",
    "\n",
    "    # Edges\n",
    "    edges = []\n",
    "    edge_attrs = []\n",
    "    for _, row in associations_df.iterrows():\n",
    "        src = job_id_map[row['job_id']]\n",
    "        dst = route_map[row['route_key']]\n",
    "        edges.append((src, dst))\n",
    "        edge_attrs.append([\n",
    "            row['transfer_time'],\n",
    "            row['throughput_gbps'],\n",
    "            row['carbon_emissions']\n",
    "        ])\n",
    "\n",
    "    # Convert to tensors\n",
    "    x = torch.tensor(job_features + route_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr), job_id_map, route_map"
   ],
   "id": "3669717f5b1d5590",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:04:46.864665Z",
     "start_time": "2025-05-11T18:04:46.856338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TemporalGNN(nn.Module):\n",
    "    def __init__(self, node_in_dim=4, edge_in_dim=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(node_in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Edge scoring head\n",
    "        self.edge_head = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # Node embeddings\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        # Edge scores\n",
    "        src, dst = edge_index\n",
    "        edge_features = torch.cat([x[src], x[dst], edge_attr], dim=1)\n",
    "        scores = self.edge_head(edge_features).squeeze(-1)\n",
    "\n",
    "        return scores"
   ],
   "id": "3f733a0467b74db4",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:04:47.764941Z",
     "start_time": "2025-05-11T18:04:47.753500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, data, associations_df, job_id_map, route_map, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Create meaningful targets\n",
    "    edge_targets = torch.zeros(data.edge_index.size(1))\n",
    "\n",
    "    # For each job, find the best route (lowest carbon that meets deadline)\n",
    "    for job_id, job_idx in job_id_map.items():\n",
    "        job_data = associations_df[associations_df['job_id'] == job_id]\n",
    "        min_carbon = job_data['carbon_emissions'].min()\n",
    "        deadline = job_data['job_deadline'].iloc[0]\n",
    "\n",
    "        # Get all edges for this job\n",
    "        edge_mask = (data.edge_index[0] == job_idx)\n",
    "        edge_indices = edge_mask.nonzero().squeeze()\n",
    "\n",
    "        # Score each edge\n",
    "        for edge_idx in edge_indices:\n",
    "            route_idx = data.edge_index[1, edge_idx].item()\n",
    "            route_key = [k for k, v in route_map.items() if v == route_idx][0]\n",
    "            edge_data = job_data[job_data['route_key'] == route_key].iloc[0]\n",
    "\n",
    "            # Target score based on carbon and deadline feasibility\n",
    "            carbon_norm = 1 - (edge_data['carbon_emissions'] - min_carbon) / job_data['carbon_emissions'].std()\n",
    "            feasible = 1.0 if edge_data['transfer_time_hours'] <= deadline else 0.0\n",
    "            edge_targets[edge_idx] = 0.7 * carbon_norm + 0.3 * feasible\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        scores = model(data)\n",
    "        loss = F.mse_loss(scores, edge_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ],
   "id": "542821c981ac8989",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:04:48.885401Z",
     "start_time": "2025-05-11T18:04:48.874150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_schedule(model, data, associations_df, job_id_map, route_map):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model(data)\n",
    "\n",
    "        # Get best route for each job\n",
    "        schedule = []\n",
    "        route_availability = defaultdict(float)\n",
    "\n",
    "        # Process jobs in deadline order\n",
    "        jobs_sorted = sorted(job_id_map.keys(),\n",
    "                             key=lambda x: associations_df[associations_df['job_id'] == x]['job_deadline'].iloc[0])\n",
    "\n",
    "        for job_id in jobs_sorted:\n",
    "            job_idx = job_id_map[job_id]\n",
    "            edge_mask = (data.edge_index[0] == job_idx)\n",
    "            edge_indices = edge_mask.nonzero().squeeze()\n",
    "\n",
    "            if len(edge_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get top scoring edge\n",
    "            best_edge = edge_indices[scores[edge_indices].argmax()]\n",
    "            route_idx = data.edge_index[1, best_edge].item()\n",
    "            route_key = [k for k, v in route_map.items() if v == route_idx][0]\n",
    "\n",
    "            # Get job data\n",
    "            job_data = associations_df[(associations_df['job_id'] == job_id) &\n",
    "                                       (associations_df['route_key'] == route_key)].iloc[0]\n",
    "\n",
    "            # Schedule job\n",
    "            start_time = max(route_availability[route_key], 0)\n",
    "            end_time = start_time + job_data['transfer_time_hours']\n",
    "\n",
    "            if end_time <= job_data['job_deadline']:\n",
    "                schedule.append({\n",
    "                    'job_id': job_id,\n",
    "                    'route_key': route_key,\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'duration': job_data['transfer_time_hours'],\n",
    "                    'carbon_emissions': job_data['carbon_emissions'],\n",
    "                    'throughput_gbps': job_data['throughput_gbps']\n",
    "                })\n",
    "                route_availability[route_key] = end_time\n",
    "\n",
    "    return pd.DataFrame(schedule)"
   ],
   "id": "43fc560ada371343",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:06:50.076705Z",
     "start_time": "2025-05-11T18:06:25.195577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "associations_df = pd.read_csv(\"data/associations_df.csv\")\n",
    "data, job_id_map, route_map = prepare_graph(associations_df)\n",
    "model = TemporalGNN()\n",
    "train_model(model, data, associations_df, job_id_map, route_map, epochs=1000)"
   ],
   "id": "f32d623f5ace4c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.2407\n",
      "Epoch 10, Loss: 5.2407\n",
      "Epoch 20, Loss: 5.2407\n",
      "Epoch 30, Loss: 5.2407\n",
      "Epoch 40, Loss: 5.2407\n",
      "Epoch 50, Loss: 5.2407\n",
      "Epoch 60, Loss: 5.2407\n",
      "Epoch 70, Loss: 5.2407\n",
      "Epoch 80, Loss: 5.2407\n",
      "Epoch 90, Loss: 5.2407\n",
      "Epoch 100, Loss: 5.2407\n",
      "Epoch 110, Loss: 5.2407\n",
      "Epoch 120, Loss: 5.2407\n",
      "Epoch 130, Loss: 5.2407\n",
      "Epoch 140, Loss: 5.2407\n",
      "Epoch 150, Loss: 5.2407\n",
      "Epoch 160, Loss: 5.2407\n",
      "Epoch 170, Loss: 5.2407\n",
      "Epoch 180, Loss: 5.2407\n",
      "Epoch 190, Loss: 5.2407\n",
      "Epoch 200, Loss: 5.2407\n",
      "Epoch 210, Loss: 5.2407\n",
      "Epoch 220, Loss: 5.2407\n",
      "Epoch 230, Loss: 5.2407\n",
      "Epoch 240, Loss: 5.2407\n",
      "Epoch 250, Loss: 5.2407\n",
      "Epoch 260, Loss: 5.2407\n",
      "Epoch 270, Loss: 5.2407\n",
      "Epoch 280, Loss: 5.2407\n",
      "Epoch 290, Loss: 5.2407\n",
      "Epoch 300, Loss: 5.2407\n",
      "Epoch 310, Loss: 5.2407\n",
      "Epoch 320, Loss: 5.2407\n",
      "Epoch 330, Loss: 5.2407\n",
      "Epoch 340, Loss: 5.2407\n",
      "Epoch 350, Loss: 5.2407\n",
      "Epoch 360, Loss: 5.2407\n",
      "Epoch 370, Loss: 5.2407\n",
      "Epoch 380, Loss: 5.2407\n",
      "Epoch 390, Loss: 5.2407\n",
      "Epoch 400, Loss: 5.2407\n",
      "Epoch 410, Loss: 5.2407\n",
      "Epoch 420, Loss: 5.2407\n",
      "Epoch 430, Loss: 5.2407\n",
      "Epoch 440, Loss: 5.2407\n",
      "Epoch 450, Loss: 5.2407\n",
      "Epoch 460, Loss: 5.2407\n",
      "Epoch 470, Loss: 5.2407\n",
      "Epoch 480, Loss: 5.2407\n",
      "Epoch 490, Loss: 5.2407\n",
      "Epoch 500, Loss: 5.2407\n",
      "Epoch 510, Loss: 5.2407\n",
      "Epoch 520, Loss: 5.2407\n",
      "Epoch 530, Loss: 5.2407\n",
      "Epoch 540, Loss: 5.2407\n",
      "Epoch 550, Loss: 5.2407\n",
      "Epoch 560, Loss: 5.2407\n",
      "Epoch 570, Loss: 5.2407\n",
      "Epoch 580, Loss: 5.2407\n",
      "Epoch 590, Loss: 5.2407\n",
      "Epoch 600, Loss: 5.2407\n",
      "Epoch 610, Loss: 5.2407\n",
      "Epoch 620, Loss: 5.2407\n",
      "Epoch 630, Loss: 5.2407\n",
      "Epoch 640, Loss: 5.2407\n",
      "Epoch 650, Loss: 5.2407\n",
      "Epoch 660, Loss: 5.2407\n",
      "Epoch 670, Loss: 5.2407\n",
      "Epoch 680, Loss: 5.2407\n",
      "Epoch 690, Loss: 5.2407\n",
      "Epoch 700, Loss: 5.2407\n",
      "Epoch 710, Loss: 5.2407\n",
      "Epoch 720, Loss: 5.2407\n",
      "Epoch 730, Loss: 5.2407\n",
      "Epoch 740, Loss: 5.2407\n",
      "Epoch 750, Loss: 5.2407\n",
      "Epoch 760, Loss: 5.2407\n",
      "Epoch 770, Loss: 5.2407\n",
      "Epoch 780, Loss: 5.2407\n",
      "Epoch 790, Loss: 5.2407\n",
      "Epoch 800, Loss: 5.2407\n",
      "Epoch 810, Loss: 5.2407\n",
      "Epoch 820, Loss: 5.2407\n",
      "Epoch 830, Loss: 5.2407\n",
      "Epoch 840, Loss: 5.2407\n",
      "Epoch 850, Loss: 5.2407\n",
      "Epoch 860, Loss: 5.2407\n",
      "Epoch 870, Loss: 5.2407\n",
      "Epoch 880, Loss: 5.2407\n",
      "Epoch 890, Loss: 5.2407\n",
      "Epoch 900, Loss: 5.2407\n",
      "Epoch 910, Loss: 5.2407\n",
      "Epoch 920, Loss: 5.2407\n",
      "Epoch 930, Loss: 5.2407\n",
      "Epoch 940, Loss: 5.2407\n",
      "Epoch 950, Loss: 5.2407\n",
      "Epoch 960, Loss: 5.2407\n",
      "Epoch 970, Loss: 5.2407\n",
      "Epoch 980, Loss: 5.2407\n",
      "Epoch 990, Loss: 5.2407\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:06:17.641066Z",
     "start_time": "2025-05-11T18:06:17.444351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "schedule = generate_schedule(model, data, associations_df, job_id_map, route_map)\n",
    "schedule.to_csv('schedules/gnn_temporal_schedule.csv')\n",
    "print(\"Optimized Schedule:\")\n",
    "print(schedule[['job_id', 'route_key', 'start_time', 'end_time', 'carbon_emissions']])\n",
    "print(f\"\\nTotal jobs scheduled: {len(schedule)}\")\n",
    "print(f\"Average carbon emissions: {schedule['carbon_emissions'].mean():.2f}\")"
   ],
   "id": "9f3f578a9046a23a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Schedule:\n",
      "     job_id route_key  start_time   end_time  carbon_emissions\n",
      "0        12  chi_buff    0.000000   0.002888          0.753953\n",
      "1        20  chi_buff    0.002888   0.005805          0.761444\n",
      "2        25  chi_buff    0.005805   0.008696          0.754853\n",
      "3        31  chi_buff    0.008696   0.011585          0.753955\n",
      "4        36  chi_buff    0.011585   0.014473          0.754055\n",
      "..      ...       ...         ...        ...               ...\n",
      "140      70  chi_buff   16.546998  16.644113         26.020229\n",
      "141     104  chi_buff   16.644113  16.755293         29.791983\n",
      "142      14  chi_buff   16.755293  23.613734       1859.153729\n",
      "143      10  chi_buff   23.613734  23.920992         82.369506\n",
      "144      59  chi_buff   23.920992  24.026950         28.391605\n",
      "\n",
      "[145 rows x 5 columns]\n",
      "\n",
      "Total jobs scheduled: 145\n",
      "Average carbon emissions: 44.80\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:29:24.503663Z",
     "start_time": "2025-05-11T18:29:17.060217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class ScheduleGNN(nn.Module):\n",
    "    def __init__(self, node_feat_dim=2, edge_feat_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Node encoders\n",
    "        self.job_encoder = nn.Sequential(\n",
    "            nn.Linear(node_feat_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.route_encoder = nn.Sequential(\n",
    "            nn.Linear(node_feat_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Graph attention\n",
    "        self.conv1 = GATv2Conv(hidden_dim, hidden_dim, edge_dim=edge_feat_dim)\n",
    "        self.conv2 = GATv2Conv(hidden_dim, hidden_dim, edge_dim=edge_feat_dim)\n",
    "\n",
    "        # Scoring network\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Encode nodes\n",
    "        h_jobs = self.job_encoder(data.x[data.job_mask])\n",
    "        h_routes = self.route_encoder(data.x[data.route_mask])\n",
    "\n",
    "        # Combine features\n",
    "        x = torch.zeros(data.num_nodes, h_jobs.size(1), device=data.x.device)\n",
    "        x[data.job_mask] = h_jobs\n",
    "        x[data.route_mask] = h_routes\n",
    "\n",
    "        # Graph processing\n",
    "        x = F.relu(self.conv1(x, data.edge_index, data.edge_attr))\n",
    "        x = F.relu(self.conv2(x, data.edge_index, data.edge_attr))\n",
    "\n",
    "        # Score edges\n",
    "        src, dst = data.edge_index\n",
    "        edge_feats = torch.cat([x[src], x[dst], data.edge_attr], dim=1)\n",
    "        return self.scorer(edge_feats).squeeze()\n",
    "\n",
    "\n",
    "def prepare_gnn_data(associations_df):\n",
    "    \"\"\"Convert dataframe to graph for GNN processing\"\"\"\n",
    "    # Create nodes\n",
    "    job_nodes = []\n",
    "    job_map = {}\n",
    "    for i, job_id in enumerate(associations_df['job_id'].unique()):\n",
    "        deadline = associations_df[associations_df['job_id'] == job_id]['job_deadline'].iloc[0]\n",
    "        job_nodes.append([deadline, 0])  # [deadline, current_time]\n",
    "        job_map[job_id] = i\n",
    "\n",
    "    route_nodes = []\n",
    "    route_map = {}\n",
    "    for i, (route, forecast) in enumerate(\n",
    "            associations_df[['route_key', 'forecast_id']].drop_duplicates().itertuples(index=False)):\n",
    "        route_data = associations_df[\n",
    "            (associations_df['route_key'] == route) &\n",
    "            (associations_df['forecast_id'] == forecast)\n",
    "            ].iloc[0]\n",
    "        route_nodes.append([route_data['throughput_gbps'], forecast])\n",
    "        route_map[(route, forecast)] = i + len(job_map)\n",
    "\n",
    "    # Create edges\n",
    "    edges = []\n",
    "    edge_attrs = []\n",
    "    for _, row in associations_df.iterrows():\n",
    "        src = job_map[row['job_id']]\n",
    "        dst = route_map.get((row['route_key'], row['forecast_id']), -1)\n",
    "        if dst >= 0:\n",
    "            edges.append((src, dst))\n",
    "            edge_attrs.append([\n",
    "                row['carbon_emissions'],\n",
    "                row['forecast_id'],\n",
    "                row['throughput_gbps']\n",
    "            ])\n",
    "\n",
    "    # Convert to tensors\n",
    "    x = torch.tensor(job_nodes + route_nodes, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "    # Create masks\n",
    "    job_mask = torch.zeros(x.size(0), dtype=torch.bool)\n",
    "    job_mask[:len(job_nodes)] = True\n",
    "    route_mask = ~job_mask\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                job_mask=job_mask, route_mask=route_mask), job_map, route_map\n",
    "\n",
    "\n",
    "def gnn_optimize(associations_df, epochs=100):\n",
    "    \"\"\"End-to-end GNN optimization pipeline\"\"\"\n",
    "    # Prepare graph data\n",
    "    data, job_map, route_map = prepare_gnn_data(associations_df)\n",
    "    model = ScheduleGNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Inverse mappings\n",
    "    idx_to_job = {v: k for k, v in job_map.items()}\n",
    "    idx_to_route = {v: k for k, v in route_map.items()}\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "\n",
    "        # Custom loss - encourages low-carbon, high-utilization schedules\n",
    "        carbon_loss = (scores.sigmoid() * data.edge_attr[:, 0]).mean()  # Carbon term\n",
    "        utilization_loss = -scores.sigmoid().mean()  # Utilization term\n",
    "        loss = carbon_loss + 0.5 * utilization_loss\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Generate final schedule\n",
    "    schedule = []\n",
    "    used_timeslots = defaultdict(set)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(data)\n",
    "        edge_order = torch.argsort(scores, descending=True)\n",
    "\n",
    "        for idx in edge_order:\n",
    "            src, dst = data.edge_index[:, idx]\n",
    "            job_id = idx_to_job[src.item()]\n",
    "            route_key, forecast = idx_to_route[dst.item()]\n",
    "\n",
    "            if (job_id not in {x['job_id'] for x in schedule}) and (forecast not in used_timeslots[route_key]):\n",
    "                schedule.append({\n",
    "                    'job_id': job_id,\n",
    "                    'route_key': route_key,\n",
    "                    'forecast_id': forecast,\n",
    "                    'carbon_emissions': data.edge_attr[idx, 0].item(),\n",
    "                    'throughput_gbps': data.edge_attr[idx, 2].item()\n",
    "                })\n",
    "                used_timeslots[route_key].add(forecast)\n",
    "\n",
    "    return pd.DataFrame(schedule)\n",
    "\n",
    "\n",
    "# Usage\n",
    "associations_df = pd.read_csv(\"data/associations_df.csv\")\n",
    "schedule = gnn_optimize(associations_df)\n",
    "\n",
    "print(\"GNN-Optimized Schedule:\")\n",
    "print(schedule.sort_values(['route_key', 'forecast_id']))\n",
    "print(f\"\\nTotal Carbon: {schedule['carbon_emissions'].sum():.2f}\")\n",
    "print(f\"Jobs Scheduled: {len(schedule)}/{len(associations_df['job_id'].unique())}\")"
   ],
   "id": "6ea6bd65e5dd8042",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 434.1384\n",
      "Epoch 10, Loss: 0.0001\n",
      "Epoch 20, Loss: 0.0000\n",
      "Epoch 30, Loss: 0.0000\n",
      "Epoch 40, Loss: 0.0000\n",
      "Epoch 50, Loss: 0.0000\n",
      "Epoch 60, Loss: 0.0000\n",
      "Epoch 70, Loss: 0.0000\n",
      "Epoch 80, Loss: 0.0000\n",
      "Epoch 90, Loss: 0.0000\n",
      "GNN-Optimized Schedule:\n",
      "     job_id  route_key  forecast_id  carbon_emissions  throughput_gbps\n",
      "0        86   chi_buff            0          0.753953     3.077484e-09\n",
      "2        47   chi_buff            1          0.768376     3.077484e-09\n",
      "4        97   chi_buff            2          0.773151     6.154967e-09\n",
      "6       107   chi_buff            3          0.768185     1.098870e-05\n",
      "8        31   chi_buff            4          0.762296     1.434103e-06\n",
      "..      ...        ...          ...               ...              ...\n",
      "137     113  tacc_buff           68        289.544586     1.157179e-01\n",
      "139      92  tacc_buff           69        455.995422     1.158106e-01\n",
      "144     117  tacc_buff           70       5991.294434     1.159787e-01\n",
      "145      14  tacc_buff           71       7319.573730     1.159815e-01\n",
      "143      30  tacc_buff           72       4901.963867     1.159748e-01\n",
      "\n",
      "[146 rows x 5 columns]\n",
      "\n",
      "Total Carbon: 27164.13\n",
      "Jobs Scheduled: 146/150\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:37:56.467173Z",
     "start_time": "2025-05-11T18:37:56.456499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_id = 1\n",
    "print(schedule[schedule['job_id'] == job_id])\n",
    "print(associations_df[(associations_df['route_key'] == 'tacc_buff') & (associations_df['forecast_id'] == 61) & (\n",
    "            associations_df['job_id'] == 1)][['carbon_emissions', 'throughput']])\n"
   ],
   "id": "73fbd1bd891f9271",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     job_id  route_key  forecast_id  carbon_emissions  throughput_gbps\n",
      "141       1  tacc_buff           61       1428.787476         0.115935\n",
      "       carbon_emissions    throughput\n",
      "11011        1428.78742  1.159353e+08\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:20:21.910384Z",
     "start_time": "2025-05-11T19:20:21.880099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class GnnPlanner:\n",
    "    def __init__(self, associations_df, job_list, model):\n",
    "        self.associations_df = associations_df\n",
    "        self.job_list = job_list\n",
    "        self.job_sizes = {job['id']: job['bytes'] for job in self.job_list}\n",
    "        self.job_deadlines = {job['id']: job['deadline'] for job in self.job_list}\n",
    "\n",
    "        self.route_list = associations_df['route_key'].unique()\n",
    "        self.time_slots = sorted(associations_df['forecast_id'].unique())\n",
    "        self.max_slot = max(self.time_slots)\n",
    "        self.epochs = 100\n",
    "        self.data, self.job_map, self.route_map = self.prepare_gnn_data()\n",
    "        self.model = model\n",
    "\n",
    "        # Store throughput (bps) and carbon for output calculations\n",
    "        self.throughput = dict(zip(\n",
    "            zip(associations_df['route_key'], associations_df['forecast_id']),\n",
    "            associations_df['throughput']  # Using throughput in bps\n",
    "        ))\n",
    "        self.carbon = dict(zip(\n",
    "            zip(associations_df['route_key'], associations_df['forecast_id']),\n",
    "            associations_df['carbon_emissions']\n",
    "        ))\n",
    "\n",
    "    def plan(self):\n",
    "        schedule = self.gnn_optimize()\n",
    "        print(\"GNN-Optimized Schedule:\")\n",
    "        print(schedule.sort_values(['job_id', 'forecast_id']))\n",
    "        print(f\"\\nTotal Carbon: {schedule['carbon_emissions'].sum():.2f}\")\n",
    "        print(f\"Total Allocated Bytes: {schedule['allocated_bytes'].sum():.2f}\")\n",
    "\n",
    "    def prepare_gnn_data(self):\n",
    "        \"\"\"Convert dataframe to graph for GNN processing\"\"\"\n",
    "        # Create nodes\n",
    "        job_nodes = []\n",
    "        job_map = {}\n",
    "        for i, job_id in enumerate(self.associations_df['job_id'].unique()):\n",
    "            job_nodes.append([\n",
    "                self.job_deadlines[job_id],  # deadline\n",
    "                self.job_sizes[job_id],  # size in bytes\n",
    "                0  # current progress\n",
    "            ])\n",
    "            job_map[job_id] = i\n",
    "\n",
    "        route_nodes = []\n",
    "        route_map = {}\n",
    "        for i, (route, forecast) in enumerate(\n",
    "                self.associations_df[['route_key', 'forecast_id']].drop_duplicates().itertuples(index=False)):\n",
    "            route_data = self.associations_df[\n",
    "                (self.associations_df['route_key'] == route) &\n",
    "                (self.associations_df['forecast_id'] == forecast)\n",
    "                ].iloc[0]\n",
    "            route_nodes.append([\n",
    "                route_data['throughput'],  # throughput in bps\n",
    "                forecast,  # time slot\n",
    "                route_data['carbon_emissions']  # carbon intensity\n",
    "            ])\n",
    "            route_map[(route, forecast)] = i + len(job_map)\n",
    "\n",
    "        print(f\"Route Nodes: {route_nodes}\")\n",
    "        print(f\"Route Maps: {route_map}\")\n",
    "        # Create edge\n",
    "        edges = []\n",
    "        edge_attrs = []\n",
    "        for _, row in self.associations_df.iterrows():\n",
    "            src = job_map[row['job_id']]\n",
    "            dst = route_map.get((row['route_key'], row['forecast_id']), -1)\n",
    "            if dst >= 0:\n",
    "                edges.append((src, dst))\n",
    "                edge_attrs.append([\n",
    "                row['carbon_intensity'],  # Just the intensity (gCO2/kWh), not total emissions\n",
    "                row['forecast_id'],\n",
    "                row['throughput'],\n",
    "                self.job_sizes[row['job_id']]\n",
    "            ])\n",
    "\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(job_nodes + route_nodes, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "        # Create masks\n",
    "        job_mask = torch.zeros(x.size(0), dtype=torch.bool)\n",
    "        job_mask[:len(job_nodes)] = True\n",
    "        route_mask = ~job_mask\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                    job_mask=job_mask, route_mask=route_mask), job_map, route_map\n",
    "\n",
    "    def gnn_optimize(self):\n",
    "        \"\"\"End-to-end GNN optimization pipeline with fractional allocations\"\"\"\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        # Inverse mappings\n",
    "        idx_to_job = {v: k for k, v in self.job_map.items()}\n",
    "        idx_to_route = {v: k for k, v in self.route_map.items()}\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            allocation_scores = self.model(self.data)\n",
    "\n",
    "            # Custom loss function\n",
    "            carbon_loss = (allocation_scores.sigmoid() * self.data.edge_attr[:, 0]).mean()\n",
    "\n",
    "            deadline_loss = self.calculate_deadline_loss(allocation_scores.sigmoid())\n",
    "            utilization_loss = -allocation_scores.sigmoid().mean()\n",
    "            print(f\"Carbon Loss: {carbon_loss}, Deadline Loss: {deadline_loss}, Utilization Loss: {utilization_loss}\")\n",
    "            loss = carbon_loss + deadline_loss + 0.5 * utilization_loss\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Generate final schedule with fractional allocations\n",
    "        schedule = []\n",
    "        job_progress = {job_id: 0.0 for job_id in self.job_map.keys()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get allocation scores for all edges\n",
    "            allocation_scores = self.model(self.data).sigmoid()\n",
    "\n",
    "            # Process each job and allocate fractions to different time slots\n",
    "            for job_idx, job_id in idx_to_job.items():\n",
    "                total_size = self.job_sizes[job_id]\n",
    "                remaining = total_size - job_progress[job_id]\n",
    "\n",
    "                if remaining <= 0:\n",
    "                    continue\n",
    "\n",
    "                # Get all possible allocations for this job\n",
    "                job_edges = (self.data.edge_index[0] == job_idx).nonzero().squeeze()\n",
    "                if job_edges.dim() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Sort edges by score and process them\n",
    "                sorted_edges = job_edges[torch.argsort(allocation_scores[job_edges], descending=True)]\n",
    "\n",
    "                for edge_idx in sorted_edges:\n",
    "                    if remaining <= 0:\n",
    "                        break\n",
    "\n",
    "                    src, dst = self.data.edge_index[:, edge_idx]\n",
    "                    route_key, forecast = idx_to_route[dst.item()]\n",
    "\n",
    "                    # Calculate maximum possible allocation for this slot\n",
    "                    # throughput is in bps, job size in bytes (8 bits per byte)\n",
    "                    max_possible_bytes = min(remaining, self.data.edge_attr[\n",
    "                        edge_idx, 2] * 3600 / 8)  # Throughput in bps * 3600s -> bits/hour -> bytes\n",
    "                    x_val = min(1.0, max_possible_bytes / (self.data.edge_attr[edge_idx, 2] * 3600 / 8))\n",
    "\n",
    "                    if x_val > 0.01:  # Only consider meaningful allocations\n",
    "                        schedule.append({\n",
    "                            'job_id': job_id,\n",
    "                            'forecast_id': int(forecast),\n",
    "                            'route_key': route_key,\n",
    "                            'allocated_fraction': x_val,\n",
    "                            'allocated_bytes': x_val * self.data.edge_attr[edge_idx, 2] * 3600 / 8,\n",
    "                            # bps * s -> bits -> bytes\n",
    "                            'carbon_emissions': x_val * self.data.edge_attr[edge_idx, 0],\n",
    "                            'completed': (job_progress[job_id] + x_val * max_possible_bytes) >= total_size * 0.99\n",
    "                        })\n",
    "\n",
    "                        job_progress[job_id] += x_val * max_possible_bytes\n",
    "                        remaining = total_size - job_progress[job_id]\n",
    "\n",
    "        return pd.DataFrame(schedule)\n",
    "\n",
    "    def calculate_deadline_loss(self, allocations):\n",
    "        \"\"\"Penalize allocations that exceed job deadlines\"\"\"\n",
    "        deadline_loss = 0.0\n",
    "        for job_idx, job_id in self.job_map.items():\n",
    "            job_edges = (self.data.edge_index[0] == job_idx).nonzero().squeeze()\n",
    "            if job_edges.dim() == 0:\n",
    "                continue\n",
    "\n",
    "            deadline = self.data.x[job_idx, 0]\n",
    "            late_allocations = allocations[job_edges] * (self.data.edge_attr[job_edges, 1] > deadline).float()\n",
    "            deadline_loss += late_allocations.sum()\n",
    "\n",
    "        return deadline_loss / len(self.job_map)\n",
    "\n",
    "\n",
    "class ScheduleGNN(nn.Module):\n",
    "    def __init__(self, node_feat_dim=3, edge_feat_dim=4, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Node encoders\n",
    "        self.job_encoder = nn.Sequential(\n",
    "            nn.Linear(node_feat_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.route_encoder = nn.Sequential(\n",
    "            nn.Linear(node_feat_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Graph attention\n",
    "        self.conv1 = GATv2Conv(hidden_dim, hidden_dim, edge_dim=edge_feat_dim)\n",
    "        self.conv2 = GATv2Conv(hidden_dim, hidden_dim, edge_dim=edge_feat_dim)\n",
    "\n",
    "        # Scoring network\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Encode nodes\n",
    "        h_jobs = self.job_encoder(data.x[data.job_mask])\n",
    "        h_routes = self.route_encoder(data.x[data.route_mask])\n",
    "\n",
    "        # Combine features\n",
    "        x = torch.zeros(data.num_nodes, h_jobs.size(1), device=data.x.device)\n",
    "        x[data.job_mask] = h_jobs\n",
    "        x[data.route_mask] = h_routes\n",
    "\n",
    "        # Graph processing\n",
    "        x = F.relu(self.conv1(x, data.edge_index, data.edge_attr))\n",
    "        x = F.relu(self.conv2(x, data.edge_index, data.edge_attr))\n",
    "\n",
    "        # Score edges\n",
    "        src, dst = data.edge_index\n",
    "        edge_feats = torch.cat([x[src], x[dst], data.edge_attr], dim=1)\n",
    "        return self.scorer(edge_feats).squeeze()"
   ],
   "id": "b06df5f66520cd5d",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:20:22.967792Z",
     "start_time": "2025-05-11T19:20:22.203536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open('config/jobs_config/150_jobs.json', 'r') as f:\n",
    "    job_data = json.load(f)\n",
    "\n",
    "planner = GnnPlanner(associations_df, job_data, ScheduleGNN())\n",
    "# planner.plan()"
   ],
   "id": "5cea3ec63981d853",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route Nodes: [[530751779.3365115, 0, 385.34340169836463], [530751779.3365115, 1, 391.20114880727095], [530751779.3365115, 2, 392.1385295392405], [530751779.3365115, 3, 389.46985934915926], [530751779.3365115, 4, 386.7971645398832], [530751779.3365115, 5, 384.01156569359387], [530751779.3365115, 6, 378.9259643536116], [530751779.3365115, 7, 372.8158641385102], [530751779.3365115, 8, 369.0922285947288], [530751779.3365115, 9, 372.6150930433119], [530751779.3365115, 10, 378.20197227270495], [530751779.3365115, 11, 384.491703083236], [530751779.3365115, 12, 379.67419056873257], [530751779.3365115, 13, 361.6185973407164], [530751779.3365115, 14, 350.168463297091], [530751779.3365115, 15, 345.0880716904645], [530751779.3365115, 16, 339.79225918773386], [530751779.3365115, 17, 333.75494047101483], [530751779.3365115, 18, 327.4079264907392], [530751779.3365115, 19, 322.55194175870804], [530751779.3365115, 20, 319.2727466585075], [530751779.3365115, 21, 325.3653930401583], [530751779.3365115, 22, 342.0879648347546], [530751779.3365115, 23, 356.3204952660115], [530751779.3365115, 24, 360.28155679717645], [530751779.3365115, 25, 359.79695671521466], [530751779.3365115, 26, 353.7234320560763], [530751779.3365115, 27, 345.09934169480124], [530751779.3365115, 28, 339.89114915329293], [530751779.3365115, 29, 340.4285415620224], [530751779.3365115, 30, 341.9464661146911], [530751779.3365115, 31, 342.45186827727986], [530751779.3365115, 32, 348.2684637272544], [530751779.3365115, 33, 356.98257093629445], [530751779.3365115, 34, 369.0685145189976], [530751779.3365115, 35, 379.7892035928172], [530751779.3365115, 36, 374.6708645230525], [530751779.3365115, 37, 360.79245944832167], [530751779.3365115, 38, 355.62302992870696], [530751779.3365115, 39, 352.83069530851253], [530751779.3365115, 40, 348.0068518020421], [530751779.3365115, 41, 342.0731984751267], [530751779.3365115, 42, 334.28805735024764], [530751779.3365115, 43, 331.40338546588896], [530751779.3365115, 44, 335.82561269968465], [530751779.3365115, 45, 345.23190819229774], [530751779.3365115, 46, 360.9268014669543], [530751779.3365115, 47, 374.8927295961846], [530751779.3365115, 48, 378.23379536088686], [530751779.3365115, 49, 375.5650197004357], [530751779.3365115, 50, 367.360169533637], [530751779.3365115, 51, 359.51803479351776], [530751779.3365115, 52, 354.08907177131454], [530751779.3365115, 53, 350.3927532302084], [530751779.3365115, 54, 348.8967084041034], [530751779.3365115, 55, 350.66480995696645], [530751779.3365115, 56, 357.78043045725167], [530751779.3365115, 57, 368.16441495079175], [530751779.3365115, 58, 378.43735829404574], [530751779.3365115, 59, 387.2137478880593], [530751779.3365115, 60, 387.6354991143239], [530751779.3365115, 61, 381.2279636439284], [530751779.3365115, 62, 381.06703315083433], [530751779.3365115, 63, 380.2458125082232], [530751779.3365115, 64, 377.061199473865], [530751779.3365115, 65, 374.94430924886166], [530751779.3365115, 66, 373.49109388927474], [530751779.3365115, 67, 372.11089492094976], [530751779.3365115, 68, 373.2790154369962], [530751779.3365115, 69, 381.1785301127574], [530751779.3365115, 70, 394.99215574805953], [530751779.3365115, 71, 395.1945228322434], [530751779.3365115, 72, 385.34340169836463], [115935337.1647996, 0, 1808.1924190936288], [115935337.1647996, 1, 1727.7463375346665], [115935337.1647996, 2, 1647.1268618086592], [115935337.1647996, 3, 1590.3207647570405], [115935337.1647996, 4, 1563.6569247825466], [115935337.1647996, 5, 1564.964840749035], [115935337.1647996, 6, 1584.091727136541], [115935337.1647996, 7, 1609.1904749581934], [115935337.1647996, 8, 1621.794067438075], [115935337.1647996, 9, 1612.3295909213366], [115935337.1647996, 10, 1582.852119779298], [115935337.1647996, 11, 1536.4221272471166], [115935337.1647996, 12, 1466.3993534521503], [115935337.1647996, 13, 1382.952469533109], [115935337.1647996, 14, 1309.9184148927548], [115935337.1647996, 15, 1261.5852853672352], [115935337.1647996, 16, 1242.377396207019], [115935337.1647996, 17, 1248.6627078107226], [115935337.1647996, 18, 1296.8891145852406], [115935337.1647996, 19, 1372.3614344220896], [115935337.1647996, 20, 1428.4478019447708], [115935337.1647996, 21, 1461.7093820651237], [115935337.1647996, 22, 1481.019526052845], [115935337.1647996, 23, 1486.2893053818318], [115935337.1647996, 24, 1469.8042864645872], [115935337.1647996, 25, 1399.7048714438015], [115935337.1647996, 26, 1336.6327144749357], [115935337.1647996, 27, 1314.365896295153], [115935337.1647996, 28, 1309.414878820253], [115935337.1647996, 29, 1323.3867884421654], [115935337.1647996, 30, 1353.8850032412415], [115935337.1647996, 31, 1386.754121660574], [115935337.1647996, 32, 1412.1552027710306], [115935337.1647996, 33, 1420.179998004753], [115935337.1647996, 34, 1416.8260168243098], [115935337.1647996, 35, 1405.4631803341238], [115935337.1647996, 36, 1383.5439567342637], [115935337.1647996, 37, 1356.855321017324], [115935337.1647996, 38, 1337.55985335908], [115935337.1647996, 39, 1333.8988260314254], [115935337.1647996, 40, 1343.0491395316512], [115935337.1647996, 41, 1370.9626805068976], [115935337.1647996, 42, 1437.700289775196], [115935337.1647996, 43, 1518.425125035088], [115935337.1647996, 44, 1570.1728848913176], [115935337.1647996, 45, 1592.1870387723102], [115935337.1647996, 46, 1601.0008392732595], [115935337.1647996, 47, 1598.5896246813095], [115935337.1647996, 48, 1565.926946018556], [115935337.1647996, 49, 1480.0421635131668], [115935337.1647996, 50, 1405.4904866522172], [115935337.1647996, 51, 1370.807057347756], [115935337.1647996, 52, 1360.6490565563647], [115935337.1647996, 53, 1368.4055298053215], [115935337.1647996, 54, 1395.5801701344765], [115935337.1647996, 55, 1435.8203968506757], [115935337.1647996, 56, 1471.180920843182], [115935337.1647996, 57, 1490.3439501611035], [115935337.1647996, 58, 1496.3960447684467], [115935337.1647996, 59, 1487.2191075188389], [115935337.1647996, 60, 1463.594740242963], [115935337.1647996, 61, 1428.787419807078], [115935337.1647996, 62, 1394.092712116018], [115935337.1647996, 63, 1374.329213154047], [115935337.1647996, 64, 1371.4630617970663], [115935337.1647996, 65, 1389.550720770443], [115935337.1647996, 66, 1463.3931966832786], [115935337.1647996, 67, 1572.365385224968], [115935337.1647996, 68, 1666.164872213619], [115935337.1647996, 69, 1733.581336480798], [115935337.1647996, 70, 1777.4397892996028], [115935337.1647996, 71, 1803.2417148867305], [115935337.1647996, 72, 1808.1924190936288]]\n",
      "Route Maps: {('chi_buff', 0): 150, ('chi_buff', 1): 151, ('chi_buff', 2): 152, ('chi_buff', 3): 153, ('chi_buff', 4): 154, ('chi_buff', 5): 155, ('chi_buff', 6): 156, ('chi_buff', 7): 157, ('chi_buff', 8): 158, ('chi_buff', 9): 159, ('chi_buff', 10): 160, ('chi_buff', 11): 161, ('chi_buff', 12): 162, ('chi_buff', 13): 163, ('chi_buff', 14): 164, ('chi_buff', 15): 165, ('chi_buff', 16): 166, ('chi_buff', 17): 167, ('chi_buff', 18): 168, ('chi_buff', 19): 169, ('chi_buff', 20): 170, ('chi_buff', 21): 171, ('chi_buff', 22): 172, ('chi_buff', 23): 173, ('chi_buff', 24): 174, ('chi_buff', 25): 175, ('chi_buff', 26): 176, ('chi_buff', 27): 177, ('chi_buff', 28): 178, ('chi_buff', 29): 179, ('chi_buff', 30): 180, ('chi_buff', 31): 181, ('chi_buff', 32): 182, ('chi_buff', 33): 183, ('chi_buff', 34): 184, ('chi_buff', 35): 185, ('chi_buff', 36): 186, ('chi_buff', 37): 187, ('chi_buff', 38): 188, ('chi_buff', 39): 189, ('chi_buff', 40): 190, ('chi_buff', 41): 191, ('chi_buff', 42): 192, ('chi_buff', 43): 193, ('chi_buff', 44): 194, ('chi_buff', 45): 195, ('chi_buff', 46): 196, ('chi_buff', 47): 197, ('chi_buff', 48): 198, ('chi_buff', 49): 199, ('chi_buff', 50): 200, ('chi_buff', 51): 201, ('chi_buff', 52): 202, ('chi_buff', 53): 203, ('chi_buff', 54): 204, ('chi_buff', 55): 205, ('chi_buff', 56): 206, ('chi_buff', 57): 207, ('chi_buff', 58): 208, ('chi_buff', 59): 209, ('chi_buff', 60): 210, ('chi_buff', 61): 211, ('chi_buff', 62): 212, ('chi_buff', 63): 213, ('chi_buff', 64): 214, ('chi_buff', 65): 215, ('chi_buff', 66): 216, ('chi_buff', 67): 217, ('chi_buff', 68): 218, ('chi_buff', 69): 219, ('chi_buff', 70): 220, ('chi_buff', 71): 221, ('chi_buff', 72): 222, ('tacc_buff', 0): 223, ('tacc_buff', 1): 224, ('tacc_buff', 2): 225, ('tacc_buff', 3): 226, ('tacc_buff', 4): 227, ('tacc_buff', 5): 228, ('tacc_buff', 6): 229, ('tacc_buff', 7): 230, ('tacc_buff', 8): 231, ('tacc_buff', 9): 232, ('tacc_buff', 10): 233, ('tacc_buff', 11): 234, ('tacc_buff', 12): 235, ('tacc_buff', 13): 236, ('tacc_buff', 14): 237, ('tacc_buff', 15): 238, ('tacc_buff', 16): 239, ('tacc_buff', 17): 240, ('tacc_buff', 18): 241, ('tacc_buff', 19): 242, ('tacc_buff', 20): 243, ('tacc_buff', 21): 244, ('tacc_buff', 22): 245, ('tacc_buff', 23): 246, ('tacc_buff', 24): 247, ('tacc_buff', 25): 248, ('tacc_buff', 26): 249, ('tacc_buff', 27): 250, ('tacc_buff', 28): 251, ('tacc_buff', 29): 252, ('tacc_buff', 30): 253, ('tacc_buff', 31): 254, ('tacc_buff', 32): 255, ('tacc_buff', 33): 256, ('tacc_buff', 34): 257, ('tacc_buff', 35): 258, ('tacc_buff', 36): 259, ('tacc_buff', 37): 260, ('tacc_buff', 38): 261, ('tacc_buff', 39): 262, ('tacc_buff', 40): 263, ('tacc_buff', 41): 264, ('tacc_buff', 42): 265, ('tacc_buff', 43): 266, ('tacc_buff', 44): 267, ('tacc_buff', 45): 268, ('tacc_buff', 46): 269, ('tacc_buff', 47): 270, ('tacc_buff', 48): 271, ('tacc_buff', 49): 272, ('tacc_buff', 50): 273, ('tacc_buff', 51): 274, ('tacc_buff', 52): 275, ('tacc_buff', 53): 276, ('tacc_buff', 54): 277, ('tacc_buff', 55): 278, ('tacc_buff', 56): 279, ('tacc_buff', 57): 280, ('tacc_buff', 58): 281, ('tacc_buff', 59): 282, ('tacc_buff', 60): 283, ('tacc_buff', 61): 284, ('tacc_buff', 62): 285, ('tacc_buff', 63): 286, ('tacc_buff', 64): 287, ('tacc_buff', 65): 288, ('tacc_buff', 66): 289, ('tacc_buff', 67): 290, ('tacc_buff', 68): 291, ('tacc_buff', 69): 292, ('tacc_buff', 70): 293, ('tacc_buff', 71): 294, ('tacc_buff', 72): 295}\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd722f5f856f35e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
